{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf46e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(\"covid19_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81618c",
   "metadata": {},
   "source": [
    "Eseguiamo un text cleaning, emoji feature extraction, emoji semantic conversation e keyword filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be08bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>emojis_unique</th>\n",
       "      <th>text_demojize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25563</th>\n",
       "      <td>biotech stocks are like literally lottery tick...</td>\n",
       "      <td>[üòÇ]</td>\n",
       "      <td>1</td>\n",
       "      <td>[üòÇ]</td>\n",
       "      <td>biotech stocks are like literally lottery tick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10781</th>\n",
       "      <td>indiafightscorona:covid19 labs in india (as on...</td>\n",
       "      <td>[üëá]</td>\n",
       "      <td>1</td>\n",
       "      <td>[üëá]</td>\n",
       "      <td>indiafightscorona:covid19 labs in india (as on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63237</th>\n",
       "      <td>well done mate. waiting for arthroscopy on my ...</td>\n",
       "      <td>[üò°]</td>\n",
       "      <td>1</td>\n",
       "      <td>[üò°]</td>\n",
       "      <td>well done mate. waiting for arthroscopy on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157545</th>\n",
       "      <td>ersa2020 web conference d-day -3!‚û°Ô∏è register n...</td>\n",
       "      <td>[‚û°]</td>\n",
       "      <td>1</td>\n",
       "      <td>[‚û°]</td>\n",
       "      <td>ersa2020 web conference d-day -3! emoji_right_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104737</th>\n",
       "      <td>üò∑ \"i am forced to celebrate my 60th birthday i...</td>\n",
       "      <td>[üò∑]</td>\n",
       "      <td>1</td>\n",
       "      <td>[üò∑]</td>\n",
       "      <td>emoji_face_with_medical_mask \"i am forced to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170502</th>\n",
       "      <td>üì¢coronavirusupdates:üìçcovid19 india tracker(as ...</td>\n",
       "      <td>[üì¢, üìç, ‚û°, ‚û°]</td>\n",
       "      <td>4</td>\n",
       "      <td>[üì¢, üìç, ‚û°, ‚û°]</td>\n",
       "      <td>emoji_loudspeakercoronavirusupdates: emoji_rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30854</th>\n",
       "      <td>and instead gave a big fu üñïüèª to covid19 preven...</td>\n",
       "      <td>[üñï, üèª]</td>\n",
       "      <td>2</td>\n",
       "      <td>[üñï, üèª]</td>\n",
       "      <td>and instead gave a big fu  emoji_middle_finger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111453</th>\n",
       "      <td>a new youth &amp;amp; covid19 üò∑report finds that 6...</td>\n",
       "      <td>[üò∑]</td>\n",
       "      <td>1</td>\n",
       "      <td>[üò∑]</td>\n",
       "      <td>a new youth &amp;amp; covid19  emoji_face_with_med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108162</th>\n",
       "      <td>‚Å¶‚Å© said at the start of lockdown ‚Äúwe are all i...</td>\n",
       "      <td>[üëá]</td>\n",
       "      <td>1</td>\n",
       "      <td>[üëá]</td>\n",
       "      <td>‚Å¶‚Å© said at the start of lockdown ‚Äúwe are all i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2790</th>\n",
       "      <td>new account, let‚Äôs follow each other and will ...</td>\n",
       "      <td>[‚ù§]</td>\n",
       "      <td>1</td>\n",
       "      <td>[‚ù§]</td>\n",
       "      <td>new account, let‚Äôs follow each other and will ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_clean        emojis  \\\n",
       "25563   biotech stocks are like literally lottery tick...           [üòÇ]   \n",
       "10781   indiafightscorona:covid19 labs in india (as on...           [üëá]   \n",
       "63237   well done mate. waiting for arthroscopy on my ...           [üò°]   \n",
       "157545  ersa2020 web conference d-day -3!‚û°Ô∏è register n...           [‚û°]   \n",
       "104737  üò∑ \"i am forced to celebrate my 60th birthday i...           [üò∑]   \n",
       "...                                                   ...           ...   \n",
       "170502  üì¢coronavirusupdates:üìçcovid19 india tracker(as ...  [üì¢, üìç, ‚û°, ‚û°]   \n",
       "30854   and instead gave a big fu üñïüèª to covid19 preven...        [üñï, üèª]   \n",
       "111453  a new youth &amp; covid19 üò∑report finds that 6...           [üò∑]   \n",
       "108162  ‚Å¶‚Å© said at the start of lockdown ‚Äúwe are all i...           [üëá]   \n",
       "2790    new account, let‚Äôs follow each other and will ...           [‚ù§]   \n",
       "\n",
       "        emoji_count emojis_unique  \\\n",
       "25563             1           [üòÇ]   \n",
       "10781             1           [üëá]   \n",
       "63237             1           [üò°]   \n",
       "157545            1           [‚û°]   \n",
       "104737            1           [üò∑]   \n",
       "...             ...           ...   \n",
       "170502            4  [üì¢, üìç, ‚û°, ‚û°]   \n",
       "30854             2        [üñï, üèª]   \n",
       "111453            1           [üò∑]   \n",
       "108162            1           [üëá]   \n",
       "2790              1           [‚ù§]   \n",
       "\n",
       "                                            text_demojize  \n",
       "25563   biotech stocks are like literally lottery tick...  \n",
       "10781   indiafightscorona:covid19 labs in india (as on...  \n",
       "63237   well done mate. waiting for arthroscopy on my ...  \n",
       "157545  ersa2020 web conference d-day -3! emoji_right_...  \n",
       "104737  emoji_face_with_medical_mask \"i am forced to c...  \n",
       "...                                                   ...  \n",
       "170502  emoji_loudspeakercoronavirusupdates: emoji_rou...  \n",
       "30854   and instead gave a big fu  emoji_middle_finger...  \n",
       "111453  a new youth &amp; covid19  emoji_face_with_med...  \n",
       "108162  ‚Å¶‚Å© said at the start of lockdown ‚Äúwe are all i...  \n",
       "2790    new account, let‚Äôs follow each other and will ...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean text\n",
    "def clean_tweet(text):\n",
    "    text=text.lower() #lower text\n",
    "    text=re.sub(r\"https?://\\S+\",\"\",text) #remove links\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "    text=re.sub(r\"@\\w+\",\"\",text) #remove mention\n",
    "    text=re.sub(r\"\\brt\\b\",\"\",text) #remove retweet\n",
    "    text=re.sub(r\"#(\\w+)\",r\"\\1\",text) # remove hashtag\n",
    "    text=re.sub(r\"[\\n\\t]\",\"\",text) # remove newline and tab\n",
    "    return text.strip()\n",
    "\n",
    "df[\"text_clean\"]=df[\"text\"].apply(clean_tweet)\n",
    "\n",
    "def has_emoji(text):\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "df[\"emojis\"]=df[\"text_clean\"].apply(extract_emojis)\n",
    "df[\"emoji_count\"]=df[\"emojis\"].apply(len)\n",
    "df[\"has_emoji\"]=df[\"emoji_count\"]>0\n",
    "\n",
    "#Transform emoji in token\n",
    "def demojize_text(text):\n",
    "    text=emoji.demojize(text,language='en')\n",
    "    text=re.sub(r\":([a-z_]+):\",r\" emoji_\\1\",text)\n",
    "    return text.strip()\n",
    "df[\"text_demojize\"]=df[\"text_clean\"].apply(demojize_text)\n",
    "\n",
    "# List of keywords to identify COVID-19 related tweets\n",
    "covid_keywords=[\"covid\",\"covid19\",\"coronavirus\",\"sars-cov-2\",\"pandemic\"]\n",
    "\n",
    "# Function to check if a text mentions COVID-19\n",
    "def is_covid_related(text):\n",
    "    pattern = r\"\\b(\" + \"|\".join(covid_keywords) + r\")\\b\"\n",
    "    return bool(re.search(pattern, text))\n",
    "\n",
    "df_with_emoji=df[df[\"has_emoji\"]].copy()\n",
    "df_with_emoji=df_with_emoji[df_with_emoji[\"text_clean\"].apply(is_covid_related)].copy()\n",
    "\n",
    "def remove_duplicate_emoji(emoji_list):\n",
    "    return list(dict.fromkeys(emoji_list))\n",
    "\n",
    "\n",
    "df_with_emoji[\"emojis_unique\"]=df_with_emoji[\"emojis\"]\n",
    "\n",
    "df_with_emoji[[\"text_clean\",\"emojis\",\"emoji_count\",\"emojis_unique\",\"text_demojize\"]].sample(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "518a83e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_no_stop</th>\n",
       "      <th>final_tokens</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54293</th>\n",
       "      <td>[isn, ‚Äô, t, everyone, technically, anti-covid1...</td>\n",
       "      <td>[everyone, technically, except, maybe, big, ph...</td>\n",
       "      <td>[everyone, technically, except, maybe, big, ph...</td>\n",
       "      <td>everyone technically except maybe big pharma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29814</th>\n",
       "      <td>[australia, :, medical, gps, ,, specialists, ,...</td>\n",
       "      <td>[australia, medical, gps, specialists, nurses,...</td>\n",
       "      <td>[australia, medical, gps, specialist, nurse, a...</td>\n",
       "      <td>australia medical gps specialist nurse ambulan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175878</th>\n",
       "      <td>[please, read, this, thread, ~, emoji_hundred_...</td>\n",
       "      <td>[please, read, thread, scamdemic, coronavirus]</td>\n",
       "      <td>[please, read, thread, scamdemic, coronavirus]</td>\n",
       "      <td>please read thread scamdemic coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53157</th>\n",
       "      <td>[august1st, it, is, the, first, time, that, ge...</td>\n",
       "      <td>[first, time, getting, closer, september, make...</td>\n",
       "      <td>[first, time, get, close, september, make, bit...</td>\n",
       "      <td>first time get close september make bit anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125522</th>\n",
       "      <td>[while, senators, go, on, vacation, until, sep...</td>\n",
       "      <td>[senators, go, vacation, sept, infected, month]</td>\n",
       "      <td>[senator, go, vacation, sept, infect, month]</td>\n",
       "      <td>senator go vacation sept infect month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32967</th>\n",
       "      <td>[emoji_loudspeakercoronavirusupdates, :, emoji...</td>\n",
       "      <td>[india, recovery, rate, crosses, improves, july]</td>\n",
       "      <td>[india, recovery, rate, cross, improve, july]</td>\n",
       "      <td>india recovery rate cross improve july</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9469</th>\n",
       "      <td>[time, emoji_watch, is, what, we, do, n't, hav...</td>\n",
       "      <td>[time, rushing, leave, legacy]</td>\n",
       "      <td>[time, rush, leave, legacy]</td>\n",
       "      <td>time rush leave legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158859</th>\n",
       "      <td>[and, they, tested, positive, emoji_face_with_...</td>\n",
       "      <td>[tested, positive, im, starting, show, symptoms]</td>\n",
       "      <td>[test, positive, im, start, show, symptom]</td>\n",
       "      <td>test positive im start show symptom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151965</th>\n",
       "      <td>[emoji_loudspeaker, pa, covid19, update, (, as...</td>\n",
       "      <td>[pa, update]</td>\n",
       "      <td>[pa, update]</td>\n",
       "      <td>pa update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34476</th>\n",
       "      <td>[seriously, ?, they, should, have, known, bett...</td>\n",
       "      <td>[seriously, known, better, wearamask, stayhome...</td>\n",
       "      <td>[seriously, know, well, wearamask, stayhomesta...</td>\n",
       "      <td>seriously know well wearamask stayhomestaysafe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tokens  \\\n",
       "54293   [isn, ‚Äô, t, everyone, technically, anti-covid1...   \n",
       "29814   [australia, :, medical, gps, ,, specialists, ,...   \n",
       "175878  [please, read, this, thread, ~, emoji_hundred_...   \n",
       "53157   [august1st, it, is, the, first, time, that, ge...   \n",
       "125522  [while, senators, go, on, vacation, until, sep...   \n",
       "32967   [emoji_loudspeakercoronavirusupdates, :, emoji...   \n",
       "9469    [time, emoji_watch, is, what, we, do, n't, hav...   \n",
       "158859  [and, they, tested, positive, emoji_face_with_...   \n",
       "151965  [emoji_loudspeaker, pa, covid19, update, (, as...   \n",
       "34476   [seriously, ?, they, should, have, known, bett...   \n",
       "\n",
       "                                           tokens_no_stop  \\\n",
       "54293   [everyone, technically, except, maybe, big, ph...   \n",
       "29814   [australia, medical, gps, specialists, nurses,...   \n",
       "175878     [please, read, thread, scamdemic, coronavirus]   \n",
       "53157   [first, time, getting, closer, september, make...   \n",
       "125522    [senators, go, vacation, sept, infected, month]   \n",
       "32967    [india, recovery, rate, crosses, improves, july]   \n",
       "9469                       [time, rushing, leave, legacy]   \n",
       "158859   [tested, positive, im, starting, show, symptoms]   \n",
       "151965                                       [pa, update]   \n",
       "34476   [seriously, known, better, wearamask, stayhome...   \n",
       "\n",
       "                                             final_tokens  \\\n",
       "54293   [everyone, technically, except, maybe, big, ph...   \n",
       "29814   [australia, medical, gps, specialist, nurse, a...   \n",
       "175878     [please, read, thread, scamdemic, coronavirus]   \n",
       "53157   [first, time, get, close, september, make, bit...   \n",
       "125522       [senator, go, vacation, sept, infect, month]   \n",
       "32967       [india, recovery, rate, cross, improve, july]   \n",
       "9469                          [time, rush, leave, legacy]   \n",
       "158859         [test, positive, im, start, show, symptom]   \n",
       "151965                                       [pa, update]   \n",
       "34476   [seriously, know, well, wearamask, stayhomesta...   \n",
       "\n",
       "                                               final_text  \n",
       "54293        everyone technically except maybe big pharma  \n",
       "29814   australia medical gps specialist nurse ambulan...  \n",
       "175878           please read thread scamdemic coronavirus  \n",
       "53157     first time get close september make bit anxious  \n",
       "125522              senator go vacation sept infect month  \n",
       "32967              india recovery rate cross improve july  \n",
       "9469                               time rush leave legacy  \n",
       "158859                test positive im start show symptom  \n",
       "151965                                          pa update  \n",
       "34476   seriously know well wearamask stayhomestaysafe...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "#Tokenize text using NLTK\n",
    "df_with_emoji[\"tokens\"]=df_with_emoji[\"text_demojize\"].apply(word_tokenize)\n",
    "\n",
    "#Remove Stopwords\n",
    "stopwords=set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    #Remove english and non-alphabetic tokens\n",
    "    return [word for word in tokens if word.isalpha() and word not in stopwords]\n",
    "df_with_emoji[\"tokens_no_stop\"]=df_with_emoji[\"tokens\"].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "#Lemmantization\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'): \n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'): \n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #Default\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    if not isinstance(tokens,list) or len(tokens) ==0:\n",
    "        return []\n",
    "    tagged_tokens=pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(word,get_wordnet_pos(pos)) for word,pos in tagged_tokens]\n",
    "\n",
    "df_with_emoji[\"final_tokens\"]=df_with_emoji[\"tokens_no_stop\"].apply(lemmatize_tokens)\n",
    "\n",
    "df_with_emoji[\"final_text\"]=df_with_emoji[\"final_tokens\"].apply(lambda x: \" \".join(x))\n",
    "df_with_emoji[[\"tokens\",\"tokens_no_stop\",\"final_tokens\",\"final_text\"]].sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3a2ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9073, 11109)\n",
      "<bound method CountVectorizer.get_feature_names_out of CountVectorizer()>\n"
     ]
    }
   ],
   "source": [
    "#Create Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorize=CountVectorizer()\n",
    "\n",
    "X=vectorize.fit_transform(df_with_emoji[\"final_text\"])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(vectorize.get_feature_names_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40be36a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Accuracy: 0.72\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.68      0.71      0.70       386\n",
      "     Neutral       0.76      0.56      0.65       632\n",
      "    Positive       0.72      0.86      0.78       797\n",
      "\n",
      "    accuracy                           0.72      1815\n",
      "   macro avg       0.72      0.71      0.71      1815\n",
      "weighted avg       0.73      0.72      0.72      1815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB,MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "\n",
    "#We generate labels using VADER (Automatically handles emohis)\n",
    "def label_sentiment(text):\n",
    "   score=analyzer.polarity_scores(text)['compound']\n",
    "   if score>0.05:\n",
    "      return \"Positive\"\n",
    "   elif score<=-0.05:\n",
    "      return \"Negative\"\n",
    "   else:\n",
    "      return \"Neutral\"\n",
    "\n",
    "df_with_emoji[\"sentiment\"]=df_with_emoji[\"text_clean\"].apply(label_sentiment)\n",
    "\n",
    "df_with_emoji[\"vader_compound\"]=df_with_emoji[\"text_clean\"].apply(lambda t: analyzer.polarity_scores(t)['compound'])\n",
    "\n",
    "custom_emoji = {\n",
    "    \"üò≠\": -2.5,\n",
    "    \"üò∑\": -1.5,\n",
    "    \"üíâ\": 0.5,\n",
    "    \"ü¶†\": -2.0,\n",
    "    \"üôè\": 1.2\n",
    "}\n",
    "def emoji_score(text):\n",
    "   raw_score=sum(custom_emoji.get(ch,0) for ch in text)\n",
    "   return np.tanh(raw_score/3)\n",
    "\n",
    "df_with_emoji[\"emoji_score\"]=df_with_emoji[\"text_clean\"].apply(emoji_score)\n",
    "\n",
    "X_text=df_with_emoji[\"text_demojize\"]\n",
    "X_extra=df_with_emoji[[\"vader_compound\",\"emoji_score\"]]\n",
    "y=df_with_emoji[\"sentiment\"] #Target labels\n",
    "\n",
    "#Training\n",
    "X_train_text,X_test_text,X_train_extra,X_test_extra,y_train,y_test=train_test_split(X_text,X_extra,y,test_size=0.2,random_state=42)\n",
    "\n",
    "#TF-IDF\n",
    "tfidf=TfidfVectorizer(ngram_range=(1,2),min_df=3,max_df=0.9,stop_words=\"english\",sublinear_tf=True)\n",
    "\n",
    "X_train_tfidf=tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf=tfidf.fit_transform(X_test_text)\n",
    "\n",
    "#Fit on training data and transform both sets into numerical matrices\n",
    "X_train_tfidf=tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf=tfidf.transform(X_test_text)\n",
    "\n",
    "#Scaling numeric feature\n",
    "scaler=MinMaxScaler()\n",
    "X_train_extra_scaled=scaler.fit_transform(X_train_extra)\n",
    "X_test_extra_scaled=scaler.fit_transform(X_test_extra)\n",
    "\n",
    "#Union+Emoji\n",
    "X_train_final=hstack([X_train_tfidf,X_train_extra_scaled])\n",
    "X_test_final=hstack([X_test_tfidf,X_test_extra_scaled])\n",
    "#Initializing and training Complement Naive Bayes Classifier\n",
    "model=ComplementNB()\n",
    "model.fit(X_train_final,y_train)\n",
    "\n",
    "#Model evaluation\n",
    "y_pred=model.predict(X_test_final)\n",
    "\n",
    "print(f\"Global Accuracy: {accuracy_score(y_test,y_pred):.2f}\\n\")\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
